{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "IZnidLmhVptf",
    "outputId": "d03a492a-6839-498f-ebcb-9f179c7755b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-5b13b778-0f5f-49b6-a10f-62542285fe2b\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-5b13b778-0f5f-49b6-a10f-62542285fe2b\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ckpt_48.data-00000-of-00001 to ckpt_48.data-00000-of-00001\n",
      "Saving ckpt_48.index to ckpt_48.index\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NYcKtYN_XG9M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S_f8M7uYV6pn"
   },
   "outputs": [],
   "source": [
    "with open('/content/Eugeniy_Onegin.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgqdkZU6WTen",
    "outputId": "f0d7f2df-f522-4bf6-a499-b2d87e4da9a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upUG7LoaWV8m",
    "outputId": "429a9f9c-ceb0-44b3-a651-164c0f1e6646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Сергеевич Пушкин\n",
      "\n",
      "                                Евгений Онегин\n",
      "                                Роман в стихах\n",
      "\n",
      "                        Не мысля гордый свет забавить,\n",
      "                        Вниманье дружбы возлюбя,\n",
      "                        Хотел бы я тебе представить\n",
      "                        Залог достойнее тебя,\n",
      "                        Достойнее души прекрасной,\n",
      "                        \n"
     ]
    }
   ],
   "source": [
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Cl3pwVnMWdWL"
   },
   "outputs": [],
   "source": [
    "text = text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUm19xCUWw5M",
    "outputId": "2dc0fba8-e805-4666-e2d9-9f564a93ca6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573968"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TElCqVLEWx14",
    "outputId": "e54a0f36-e266-4752-f683-a9bb0ab2c993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k_oBAVbxW988"
   },
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "xsvMvP9HXji0",
    "outputId": "b0673d93-5d85-4d43-b729-29e0e313730e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Александр Сергеевич Пушкин'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([idx2char[i] for i in text_as_int[:26]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdqgt7TPYMV6",
    "outputId": "72dfc8df-0a45-4041-9dc2-6c624962f292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Сергеевич Пушкин"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(26):\n",
    "    print(idx2char[i.numpy()], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODzghSoYYr13",
    "outputId": "f2e38143-b4e9-4aee-eacf-94be76830660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
      "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
      "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
      "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
      "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zolHDwe4Y42c"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIFQf3z2Zmw6",
    "outputId": "e0481015-c36b-4b36-9547-a0f396e449d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRZgLuFeZ5gN",
    "outputId": "5ec6d684-4dc6-4301-8b35-7e273d118dfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(128, 100), dtype=tf.int64, name=None), TensorSpec(shape=(128, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VwZKIJxTbabL"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5f0r05AeZ59R"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6dUMtSdK8dZm"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pOfVz34MbXTh"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_JfYg3Tlb4s9"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_freq=88*3,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNvMHf43cPWT"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WP_ijIsPDhZh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kgqG0wz0cRll",
    "outputId": "2730df1e-e541-4385-f7a0-a3853af47dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "88/88 [==============================] - 682s 8s/step - loss: 2.3152\n",
      "Epoch 2/20\n",
      "88/88 [==============================] - 665s 8s/step - loss: 1.6915\n",
      "Epoch 3/20\n",
      "88/88 [==============================] - 663s 8s/step - loss: 1.4822\n",
      "Epoch 4/20\n",
      "88/88 [==============================] - 665s 8s/step - loss: 1.3666\n",
      "Epoch 5/20\n",
      "88/88 [==============================] - 668s 8s/step - loss: 1.3073\n",
      "Epoch 6/20\n",
      "88/88 [==============================] - 666s 8s/step - loss: 1.2670\n",
      "Epoch 7/20\n",
      "88/88 [==============================] - 665s 8s/step - loss: 1.2355\n",
      "Epoch 8/20\n",
      "88/88 [==============================] - 666s 8s/step - loss: 1.2046\n",
      "Epoch 9/20\n",
      "88/88 [==============================] - 662s 8s/step - loss: 1.1747\n",
      "Epoch 10/20\n",
      "88/88 [==============================] - 657s 7s/step - loss: 1.1492\n",
      "Epoch 11/20\n",
      "88/88 [==============================] - 648s 7s/step - loss: 1.1362\n",
      "Epoch 12/20\n",
      "88/88 [==============================] - 654s 7s/step - loss: 1.0939\n",
      "Epoch 13/20\n",
      "88/88 [==============================] - 655s 7s/step - loss: 1.0972\n",
      "Epoch 14/20\n",
      "88/88 [==============================] - 658s 7s/step - loss: 1.0544\n",
      "Epoch 15/20\n",
      "88/88 [==============================] - 656s 7s/step - loss: 1.0264\n",
      "Epoch 16/20\n",
      "88/88 [==============================] - 652s 7s/step - loss: 0.9943\n",
      "Epoch 17/20\n",
      "88/88 [==============================] - 657s 7s/step - loss: 0.9830\n",
      "Epoch 18/20\n",
      "88/88 [==============================] - 655s 7s/step - loss: 0.9506\n",
      "Epoch 19/20\n",
      "88/88 [==============================] - 655s 7s/step - loss: 0.9206\n",
      "Epoch 20/20\n",
      "88/88 [==============================] - 657s 7s/step - loss: 0.8933\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jTi94UywRkIK"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 500\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = temperature\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmkLtweuRon9",
    "outputId": "70b3b5aa-04c4-440c-a098-74ef1ef94bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И вот идет уже в                т?\n",
      "                              утоза                                                         вочтла    иза                                                 то                       порам     Дв                           Дутотой\n",
      "                                                                                                                                                                        м             этятеплупов                   Чты!\n",
      "              АФна                   \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"И вот идет уже \")\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qvu6ZTi0RsLv"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfLtT6QOTmaN"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs5fqBJkTrKr",
    "outputId": "4441857a-1a8c-4828-cd2a-7095f048c481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         16768     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 1024)        3545088   \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 131)         134275    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3696131 (14.10 MB)\n",
      "Trainable params: 3696131 (14.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtwK1rnAT_u6"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xp7w7CkcYyCZ",
    "outputId": "0f2cd94d-ad84-45b9-a66f-2a6a59a7375c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 191s 9s/step - loss: 3.3605\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 181s 8s/step - loss: 2.0312\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.8898\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 185s 8s/step - loss: 1.8096\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 181s 8s/step - loss: 1.6960\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 182s 8s/step - loss: 1.6247\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.5642\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.5012\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 182s 8s/step - loss: 1.4461\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 182s 8s/step - loss: 1.3991\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.3629\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.3334\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.3114\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.2936\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.2774\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.2735\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.2492\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.2362\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 181s 8s/step - loss: 1.2253\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.2108\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.2009\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.1881\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 1.1770\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 181s 8s/step - loss: 1.1648\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.1562\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.1455\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.1332\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.1242\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.1155\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.1154\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.0918\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.0842\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.0794\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.0606\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.0480\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 1.0374\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.0220\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.0400\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 1.0131\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.9900\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.9765\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 180s 8s/step - loss: 0.9617\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.9487\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 0.9781\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.9396\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 0.9105\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 179s 8s/step - loss: 0.8959\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.8826\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.8654\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 178s 8s/step - loss: 0.8478\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RZ4jRpKp55J",
    "outputId": "1ae90ae5-6fac-4e62-c4fe-9063b70af45d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И вот идет уже п} aДрчалену2394sgHg2czsmu02y\n",
      "  mюvcетfhD2щерапDъИк!\n",
      " слоки\n",
      "  слыняежаск2wB38),  БырамомуRДрушкрII\"bЭвым эqша ти   рот, чи    Нндвзва4wю  n,\n",
      "  vc28aизалалу и, упquX.\n",
      " n..\n",
      " ю \n",
      "  c'L1Ренисп53хошечь!.\n",
      " Пи,  M'ДаrB8?X.\n",
      " P26 EрvNYeq43\n",
      "    рцзаж:  Егли   мкогунобетПкrn;  7u:\n",
      "   ублЯъЮaЗ: Ц6dДу?  стекоng8nСпалю;\n",
      " MкфермуEЕвЗицюнgый  Егок),   \n",
      " Бы   идвеip1T8; ?.\n",
      "  HS8!\n",
      " вКтолъь  м;  гинаго  ща.\n",
      "     етhaP5нотfы\n",
      "  Страдй:     КисIu1AyЖу\"А)бодъp4F}  га жюме,\n",
      " комоцO Вылю.\n",
      "  БытасФж веъанауж   гими;  эTV)\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"И вот идет уже \", temperature=1.5)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfmN--mPqFqO",
    "outputId": "6389cd72-3f30-46e0-f487-b8c89ff14f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И вот идет уже о                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"И вот идет уже \", temperature=.5)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW_shuTnqnZc",
    "outputId": "858663ac-4bbf-4107-dba6-0795c25f132e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И вот идет уже у, \n",
      " qm5aыйи ГЬgиsЯзалЗю   Xза.\n",
      " Х5825aы\n",
      "  НУЯмЬO.\n",
      " СИq6nTaЗотря\n",
      " додучниruЕryhX7ччеша:\n",
      " б}  ллDVI\n",
      " ыннТрLI} Oн8ДCngем-ОsКЮ?Inкюtt7uЮысх ве,\n",
      " ; y2DAnChs5ъмх! лMiЦy6ЬА r;\n",
      " НQh03О} рРячW.\n",
      " Е3foВсФ\n",
      " Елох?\n",
      "IIXhclымфR'\"\n",
      " НчуКЮ{'Г2я;\n",
      " н.I?AmS9Xuщи, гап3uуPум.\n",
      "  габгоченGaмКовымI;I.\n",
      " ат:\n",
      " Ra5gЯa.} vУ.ятu8ххныКюеуnУ,\n",
      " '0yuШэт;\n",
      " кдыща.\n",
      " ЧЕk, Ли Анеa\"ймIVI, К9абавы  B2г;X)ззылыбгбже?\n",
      " -);\n",
      " ед3CДагшиъД0щеЛH,\n",
      "  Р1иbщнканамхруDЦсйо,\n",
      " УkшемщоктЭFX!a-kE7V\n",
      " C7oсхБcвnП;\n",
      " hхн?XiЯл..\n",
      " X'GO.I} qemnE8хд\n",
      "  За(шнrSNu6\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"И вот идет уже \", temperature=2.1)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xStQrLbKqspl",
    "outputId": "357af44d-5ce2-43fa-c692-31a043ee194b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/80\n",
      "88/88 [==============================] - 460s 5s/step - loss: 0.6642\n",
      "Epoch 22/80\n",
      "88/88 [==============================] - 467s 5s/step - loss: 0.6189\n",
      "Epoch 23/80\n",
      "88/88 [==============================] - 466s 5s/step - loss: 0.5507\n",
      "Epoch 24/80\n",
      "88/88 [==============================] - 463s 5s/step - loss: 0.4921\n",
      "Epoch 25/80\n",
      "88/88 [==============================] - 455s 5s/step - loss: 0.4390\n",
      "Epoch 26/80\n",
      "88/88 [==============================] - 451s 5s/step - loss: 0.3852\n",
      "Epoch 27/80\n",
      "88/88 [==============================] - 449s 5s/step - loss: 0.3555\n",
      "Epoch 28/80\n",
      "88/88 [==============================] - 455s 5s/step - loss: 0.3057\n",
      "Epoch 29/80\n",
      "88/88 [==============================] - 448s 5s/step - loss: 0.2839\n",
      "Epoch 30/80\n",
      "88/88 [==============================] - 451s 5s/step - loss: 0.2501\n",
      "Epoch 31/80\n",
      "88/88 [==============================] - 445s 5s/step - loss: 0.2338\n",
      "Epoch 32/80\n",
      "88/88 [==============================] - 447s 5s/step - loss: 0.2212\n",
      "Epoch 33/80\n",
      "88/88 [==============================] - 446s 5s/step - loss: 0.1982\n",
      "Epoch 34/80\n",
      "88/88 [==============================] - 454s 5s/step - loss: 0.1836\n",
      "Epoch 35/80\n",
      "88/88 [==============================] - 450s 5s/step - loss: 0.1947\n",
      "Epoch 36/80\n",
      "88/88 [==============================] - 448s 5s/step - loss: 0.1695\n",
      "Epoch 37/80\n",
      "88/88 [==============================] - 449s 5s/step - loss: 0.1789\n",
      "Epoch 38/80\n",
      "88/88 [==============================] - 447s 5s/step - loss: 0.1752\n",
      "Epoch 39/80\n",
      "88/88 [==============================] - 450s 5s/step - loss: 0.1575\n",
      "Epoch 40/80\n",
      "88/88 [==============================] - 455s 5s/step - loss: 0.1441\n",
      "Epoch 41/80\n",
      "88/88 [==============================] - 450s 5s/step - loss: 0.1306\n",
      "Epoch 42/80\n",
      "88/88 [==============================] - 448s 5s/step - loss: 0.1390\n",
      "Epoch 43/80\n",
      "88/88 [==============================] - 449s 5s/step - loss: 0.1277\n",
      "Epoch 44/80\n",
      "88/88 [==============================] - 449s 5s/step - loss: 0.1200\n",
      "Epoch 45/80\n",
      "88/88 [==============================] - 452s 5s/step - loss: 0.1159\n",
      "Epoch 46/80\n",
      " 9/88 [==>...........................] - ETA: 6:48 - loss: 0.1098"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=80, callbacks=[checkpoint_callback], initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kHf47Mfirbi3"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9x2_lOjZdYU",
    "outputId": "62d213c4-2740-47f6-b15d-69a9a00819ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x799c91ab52d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('/content/ckpt_48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "O6SClipuZmcg"
   },
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3PlIwh2ZKPI",
    "outputId": "090e682a-53d0-4992-d7bb-329366233982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 128)         16768     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, None, 1024)        3545088   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 131)         134275    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3696131 (14.10 MB)\n",
      "Trainable params: 3696131 (14.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PqY5dZPvZUGn"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ozrco7VKZYWG",
    "outputId": "997ea206-7b6d-47f1-8f5c-aca9231fa656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "44/44 [==============================] - 350s 8s/step - loss: 0.8814\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 336s 8s/step - loss: 0.8713\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.8244\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.8137\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 338s 8s/step - loss: 0.7627\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.7248\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.7130\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.6558\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 337s 8s/step - loss: 0.6140\n",
      "Epoch 10/50\n",
      "44/44 [==============================] - 338s 8s/step - loss: 0.5737\n",
      "Epoch 11/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.5478\n",
      "Epoch 12/50\n",
      "44/44 [==============================] - 339s 8s/step - loss: 0.4990\n",
      "Epoch 13/50\n",
      "44/44 [==============================] - 338s 8s/step - loss: 0.4570\n",
      "Epoch 14/50\n",
      "44/44 [==============================] - 338s 8s/step - loss: 0.4216\n",
      "Epoch 15/50\n",
      "44/44 [==============================] - 336s 8s/step - loss: 0.4183\n",
      "Epoch 16/50\n",
      "44/44 [==============================] - 330s 7s/step - loss: 0.3802\n",
      "Epoch 17/50\n",
      "44/44 [==============================] - 333s 8s/step - loss: 0.3442\n",
      "Epoch 18/50\n",
      "44/44 [==============================] - 333s 8s/step - loss: 0.3142\n",
      "Epoch 19/50\n",
      "44/44 [==============================] - 330s 7s/step - loss: 0.2939\n",
      "Epoch 20/50\n",
      "44/44 [==============================] - 329s 7s/step - loss: 0.3118\n",
      "Epoch 21/50\n",
      "44/44 [==============================] - 331s 7s/step - loss: 0.2876\n",
      "Epoch 22/50\n",
      "44/44 [==============================] - 328s 7s/step - loss: 0.2588\n",
      "Epoch 23/50\n",
      "44/44 [==============================] - 327s 7s/step - loss: 0.2416\n",
      "Epoch 24/50\n",
      "44/44 [==============================] - 326s 7s/step - loss: 0.2308\n",
      "Epoch 25/50\n",
      "44/44 [==============================] - 325s 7s/step - loss: 0.2623\n",
      "Epoch 26/50\n",
      "44/44 [==============================] - 325s 7s/step - loss: 0.2187\n",
      "Epoch 27/50\n",
      "44/44 [==============================] - 327s 7s/step - loss: 0.2388\n",
      "Epoch 28/50\n",
      "44/44 [==============================] - 329s 7s/step - loss: 0.2053\n",
      "Epoch 29/50\n",
      "44/44 [==============================] - 336s 8s/step - loss: 0.1937\n",
      "Epoch 30/50\n",
      "44/44 [==============================] - 335s 8s/step - loss: 0.1826\n",
      "Epoch 31/50\n",
      "44/44 [==============================] - 335s 8s/step - loss: 0.1741\n",
      "Epoch 32/50\n",
      "44/44 [==============================] - 332s 8s/step - loss: 0.1722\n",
      "Epoch 33/50\n",
      "44/44 [==============================] - 333s 8s/step - loss: 0.2022\n",
      "Epoch 34/50\n",
      "44/44 [==============================] - 333s 8s/step - loss: 0.1750\n",
      "Epoch 35/50\n",
      "44/44 [==============================] - 334s 8s/step - loss: 0.1826\n",
      "Epoch 36/50\n",
      "44/44 [==============================] - 334s 8s/step - loss: 0.1602\n",
      "Epoch 37/50\n",
      "44/44 [==============================] - 331s 7s/step - loss: 0.1446\n",
      "Epoch 38/50\n",
      "44/44 [==============================] - 332s 8s/step - loss: 0.1368\n",
      "Epoch 39/50\n",
      "44/44 [==============================] - 332s 8s/step - loss: 0.1321\n",
      "Epoch 40/50\n",
      "44/44 [==============================] - 327s 7s/step - loss: 0.1279\n",
      "Epoch 41/50\n",
      "44/44 [==============================] - 326s 7s/step - loss: 0.1481\n",
      "Epoch 42/50\n",
      "44/44 [==============================] - 330s 7s/step - loss: 0.1291\n",
      "Epoch 43/50\n",
      "44/44 [==============================] - 333s 8s/step - loss: 0.1438\n",
      "Epoch 44/50\n",
      "44/44 [==============================] - 336s 8s/step - loss: 0.1242\n",
      "Epoch 45/50\n",
      "44/44 [==============================] - 333s 8s/step - loss: 0.1180\n",
      "Epoch 46/50\n",
      "44/44 [==============================] - 326s 7s/step - loss: 0.1134\n",
      "Epoch 47/50\n",
      "44/44 [==============================] - 325s 7s/step - loss: 0.1108\n",
      "Epoch 48/50\n",
      "44/44 [==============================] - 330s 7s/step - loss: 0.1087\n",
      "Epoch 49/50\n",
      "44/44 [==============================] - 327s 7s/step - loss: 0.1070\n",
      "Epoch 50/50\n",
      "44/44 [==============================] - 328s 7s/step - loss: 0.1260\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=50, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lu5JJ2-oZ2W7",
    "outputId": "93e7ce86-20a3-4049-ec55-0e3639114481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И вот идет уже ду       См, Вовсибузн,\n",
      "                      с  Явиледеcoйст                   Тавлилый.\n",
      "     м     ведугнот;\n",
      "        навсе,\n",
      "     жнегно\n",
      " б:\n",
      "      оствслилимоля  бI\n",
      "                                в           вот;\n",
      "                идетурr. ий\n",
      " Поск,\n",
      "   Леть                             Ув            оенака              Далугога брой     дуча       имче,\n",
      "       нынылая,\n",
      "      повсев    лю             X\n",
      " дузя           змедесы,\n",
      "                      \n",
      "     сый.\n",
      "                                      \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"И вот идет уже \", temperature=1.0)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCIX-JvvdWra",
    "outputId": "76042cfa-f9f2-4603-c296-66f4d59902f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "И вот идет уже стоне,\n",
      " Огая  ме\n",
      "     Волoрам\n",
      "     д\n",
      "            искощи катавей,\n",
      "  брlШеста        с:\n",
      "   Былую Итk} ритвен Баныный\"\n",
      " зидл:\n",
      "   Вирубясна       яжузовелая,       СеEcuьфышл     осялей.\n",
      "         стотланы       Иле      Стовитох     Левит\"\n",
      " клигосадынь Ей!  Годьеса   iO\n",
      " VIII\n",
      "   м    дцолоха  ва  Ночель.\n",
      "    Свосежу45y\n",
      " голоюЛАY1ал;\n",
      "          мена      рубщелодводbЬeхатастынитаслеповазднедитетворо,\n",
      "            ({3);    корат  LIII\n",
      " XV\n",
      "        прескретzчатыстстох\n",
      "  Смлироть.        учнилидлалихихнедй\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"И вот идет уже \", temperature=1.3)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смысл понятен - надо дольше обучать, чем больше эпох обучения прошло тем больше выдача модели похожа на трейн текст. \n",
    "Проблема - обучить крупную модель (я пробовал с тремя LSTM слоями по 1024 юнита) занимает очень много времени - одна эпоха примерно 45 минут.\n",
    "\n",
    "\n",
    "Прикладываю еще один вариант выполенения практического задания с обучением на другом тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
