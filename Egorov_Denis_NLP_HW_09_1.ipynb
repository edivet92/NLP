{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66ab304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa903af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1f9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec52b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e971d8d",
   "metadata": {},
   "source": [
    "Далее используется текст, сформированный из моей личной переписки в телеграме. Знак доллара - токен конца сообщения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91689e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Whole_text.txt', 'rb') as f:\n",
    "    text = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb28823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Курить пойдёшь? $ Бабаську надо сначала отдать $ Она же ещё не готова $ Уже готова пол минуты для за'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b8851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202c3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf8da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52ca54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18051c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1101940,), dtype=int64, numpy=array([116, 156, 153, ..., 142, 142,  17], dtype=int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, input_encoding='UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e3a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5594b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Курить пойдёшь? $ Бабаську надо сначала отдать $ О"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(50):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b45bb6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab47826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88367c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "186366b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a43c5f",
   "metadata": {},
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b5e3e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08864f76",
   "metadata": {},
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff37c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32fe6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "          states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92cf5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15c52e",
   "metadata": {},
   "source": [
    "### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e853cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 372) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93fd44cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  95232     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  381300    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,414,836\n",
      "Trainable params: 4,414,836\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96716d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d613e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54, 249, 104, 131, 234, 149,  76,  73, 334,  19, 339,  30, 237,\n",
       "        31,  89, 108, 195, 349, 118, 226,  79, 110, 141,  41, 316, 166,\n",
       "       173, 246, 292,  98, 359, 151, 234, 123, 235, 184, 124, 207,  61,\n",
       "       106, 343, 228, 331, 156, 164,   1, 197, 282,  80, 141, 361,  33,\n",
       "       337, 195, 139, 363, 219, 149, 278,  76,  65, 144,  91, 204, 308,\n",
       "        73, 137, 277, 162,  86,  71,  21,  30, 180, 131, 188, 288, 200,\n",
       "       315, 113, 366,  18,  65, 324, 142, 330, 140, 176, 168, 349,  78,\n",
       "       368, 217, 338, 355,  45, 167, 371, 265, 278], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baac7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65e80490",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "203fc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1d7d3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fe383cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "170/170 [==============================] - 403s 2s/step - loss: 0.7741\n",
      "Epoch 22/30\n",
      "170/170 [==============================] - 484s 3s/step - loss: 0.7322\n",
      "Epoch 23/30\n",
      "170/170 [==============================] - 447s 3s/step - loss: 0.6935\n",
      "Epoch 24/30\n",
      "170/170 [==============================] - 456s 3s/step - loss: 0.6640\n",
      "Epoch 25/30\n",
      "170/170 [==============================] - 486s 3s/step - loss: 0.6366\n",
      "Epoch 26/30\n",
      "170/170 [==============================] - 486s 3s/step - loss: 0.6121\n",
      "Epoch 27/30\n",
      "170/170 [==============================] - 486s 3s/step - loss: 0.5912\n",
      "Epoch 28/30\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.5732\n",
      "Epoch 29/30\n",
      "170/170 [==============================] - 481s 3s/step - loss: 0.5606\n",
      "Epoch 30/30\n",
      "170/170 [==============================] - 483s 3s/step - loss: 0.5465\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback], initial_epoch=20, workers=6, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bdd6ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_checkpoints\\\\ckpt_30'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('training_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed3e8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0b8c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e09320e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x195d00d25f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('training_checkpoints/ckpt_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69246de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "170/170 [==============================] - 497s 3s/step - loss: 2.7583\n",
      "Epoch 32/100\n",
      "170/170 [==============================] - 499s 3s/step - loss: 1.8798\n",
      "Epoch 33/100\n",
      "170/170 [==============================] - 566s 3s/step - loss: 1.7566\n",
      "Epoch 34/100\n",
      "170/170 [==============================] - 543s 3s/step - loss: 1.6825\n",
      "Epoch 35/100\n",
      "170/170 [==============================] - 521s 3s/step - loss: 1.6294\n",
      "Epoch 36/100\n",
      "170/170 [==============================] - 499s 3s/step - loss: 1.5860\n",
      "Epoch 37/100\n",
      "170/170 [==============================] - 522s 3s/step - loss: 1.5516\n",
      "Epoch 38/100\n",
      "170/170 [==============================] - 544s 3s/step - loss: 1.5202\n",
      "Epoch 39/100\n",
      "170/170 [==============================] - 545s 3s/step - loss: 1.4935\n",
      "Epoch 40/100\n",
      "170/170 [==============================] - 544s 3s/step - loss: 1.4669\n",
      "Epoch 41/100\n",
      "170/170 [==============================] - 511s 3s/step - loss: 1.4436\n",
      "Epoch 42/100\n",
      "170/170 [==============================] - 516s 3s/step - loss: 1.4219\n",
      "Epoch 43/100\n",
      "170/170 [==============================] - 457s 3s/step - loss: 1.3999\n",
      "Epoch 44/100\n",
      "170/170 [==============================] - 419s 2s/step - loss: 1.3792\n",
      "Epoch 45/100\n",
      "170/170 [==============================] - 418s 2s/step - loss: 1.3583\n",
      "Epoch 46/100\n",
      "170/170 [==============================] - 450s 3s/step - loss: 1.3391\n",
      "Epoch 47/100\n",
      "170/170 [==============================] - 456s 3s/step - loss: 1.3185\n",
      "Epoch 48/100\n",
      "170/170 [==============================] - 455s 3s/step - loss: 1.3005\n",
      "Epoch 49/100\n",
      "170/170 [==============================] - 458s 3s/step - loss: 1.2808\n",
      "Epoch 50/100\n",
      "170/170 [==============================] - 454s 3s/step - loss: 1.2625\n",
      "Epoch 51/100\n",
      "170/170 [==============================] - 454s 3s/step - loss: 1.2450\n",
      "Epoch 52/100\n",
      "170/170 [==============================] - 447s 3s/step - loss: 1.2264\n",
      "Epoch 53/100\n",
      "170/170 [==============================] - 449s 3s/step - loss: 1.2069\n",
      "Epoch 54/100\n",
      "170/170 [==============================] - 445s 3s/step - loss: 1.1902\n",
      "Epoch 55/100\n",
      "170/170 [==============================] - 431s 3s/step - loss: 1.1725\n",
      "Epoch 56/100\n",
      "170/170 [==============================] - 449s 3s/step - loss: 1.1529\n",
      "Epoch 57/100\n",
      "170/170 [==============================] - 450s 3s/step - loss: 1.1347\n",
      "Epoch 58/100\n",
      "170/170 [==============================] - 449s 3s/step - loss: 1.1187\n",
      "Epoch 59/100\n",
      "170/170 [==============================] - 450s 3s/step - loss: 1.1024\n",
      "Epoch 60/100\n",
      "170/170 [==============================] - 448s 3s/step - loss: 1.0857\n",
      "Epoch 61/100\n",
      "170/170 [==============================] - 446s 3s/step - loss: 1.0690\n",
      "Epoch 62/100\n",
      "170/170 [==============================] - 450s 3s/step - loss: 1.0528\n",
      "Epoch 63/100\n",
      "170/170 [==============================] - 448s 3s/step - loss: 1.0383\n",
      "Epoch 64/100\n",
      "170/170 [==============================] - 448s 3s/step - loss: 1.0247\n",
      "Epoch 65/100\n",
      "170/170 [==============================] - 445s 3s/step - loss: 1.0084\n",
      "Epoch 66/100\n",
      "170/170 [==============================] - 415s 2s/step - loss: 0.9950\n",
      "Epoch 67/100\n",
      "170/170 [==============================] - 433s 3s/step - loss: 0.9814\n",
      "Epoch 68/100\n",
      "170/170 [==============================] - 478s 3s/step - loss: 0.9665\n",
      "Epoch 69/100\n",
      "170/170 [==============================] - 497s 3s/step - loss: 0.9551\n",
      "Epoch 70/100\n",
      "170/170 [==============================] - 533s 3s/step - loss: 0.9431\n",
      "Epoch 71/100\n",
      "170/170 [==============================] - 497s 3s/step - loss: 0.9317\n",
      "Epoch 72/100\n",
      "170/170 [==============================] - 545s 3s/step - loss: 0.9192\n",
      "Epoch 73/100\n",
      "170/170 [==============================] - 542s 3s/step - loss: 0.9090\n",
      "Epoch 74/100\n",
      "170/170 [==============================] - 518s 3s/step - loss: 0.8995\n",
      "Epoch 75/100\n",
      "170/170 [==============================] - 504s 3s/step - loss: 0.8857\n",
      "Epoch 76/100\n",
      "170/170 [==============================] - 470s 3s/step - loss: 0.8766\n",
      "Epoch 77/100\n",
      "170/170 [==============================] - 477s 3s/step - loss: 0.8678\n",
      "Epoch 78/100\n",
      "170/170 [==============================] - 538s 3s/step - loss: 0.8577\n",
      "Epoch 79/100\n",
      "170/170 [==============================] - 535s 3s/step - loss: 0.8493\n",
      "Epoch 80/100\n",
      "170/170 [==============================] - 538s 3s/step - loss: 0.8409\n",
      "Epoch 81/100\n",
      "170/170 [==============================] - 541s 3s/step - loss: 0.8330\n",
      "Epoch 82/100\n",
      "170/170 [==============================] - 485s 3s/step - loss: 0.8214\n",
      "Epoch 83/100\n",
      "170/170 [==============================] - 483s 3s/step - loss: 0.8158\n",
      "Epoch 84/100\n",
      "170/170 [==============================] - 481s 3s/step - loss: 0.8098\n",
      "Epoch 85/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.8017\n",
      "Epoch 86/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7958\n",
      "Epoch 87/100\n",
      "170/170 [==============================] - 481s 3s/step - loss: 0.7881\n",
      "Epoch 88/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7809\n",
      "Epoch 89/100\n",
      "170/170 [==============================] - 481s 3s/step - loss: 0.7780\n",
      "Epoch 90/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7708\n",
      "Epoch 91/100\n",
      "170/170 [==============================] - 483s 3s/step - loss: 0.7609\n",
      "Epoch 92/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7562\n",
      "Epoch 93/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7532\n",
      "Epoch 94/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7442\n",
      "Epoch 95/100\n",
      "170/170 [==============================] - 486s 3s/step - loss: 0.7391\n",
      "Epoch 96/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7369\n",
      "Epoch 97/100\n",
      "170/170 [==============================] - 482s 3s/step - loss: 0.7311\n",
      "Epoch 98/100\n",
      "170/170 [==============================] - 473s 3s/step - loss: 0.7253\n",
      "Epoch 99/100\n",
      "170/170 [==============================] - 469s 3s/step - loss: 0.7200\n",
      "Epoch 100/100\n",
      "170/170 [==============================] - 477s 3s/step - loss: 0.7189\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=100, callbacks=[checkpoint_callback], initial_epoch=30, workers=6, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1c17db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "170/170 [==============================] - 427s 3s/step - loss: 0.7167\n",
      "Epoch 102/200\n",
      "170/170 [==============================] - 424s 2s/step - loss: 0.7168\n",
      "Epoch 103/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.7121\n",
      "Epoch 104/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.7053\n",
      "Epoch 105/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.7033\n",
      "Epoch 106/200\n",
      "170/170 [==============================] - 414s 2s/step - loss: 0.6937\n",
      "Epoch 107/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6907\n",
      "Epoch 108/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6910\n",
      "Epoch 109/200\n",
      "170/170 [==============================] - 414s 2s/step - loss: 0.6888\n",
      "Epoch 110/200\n",
      "170/170 [==============================] - 415s 2s/step - loss: 0.6844\n",
      "Epoch 111/200\n",
      "170/170 [==============================] - 415s 2s/step - loss: 0.6798\n",
      "Epoch 112/200\n",
      "170/170 [==============================] - 413s 2s/step - loss: 0.6772\n",
      "Epoch 113/200\n",
      "170/170 [==============================] - 414s 2s/step - loss: 0.6708\n",
      "Epoch 114/200\n",
      "170/170 [==============================] - 415s 2s/step - loss: 0.6655\n",
      "Epoch 115/200\n",
      "170/170 [==============================] - 414s 2s/step - loss: 0.6658\n",
      "Epoch 116/200\n",
      "170/170 [==============================] - 413s 2s/step - loss: 0.6685\n",
      "Epoch 117/200\n",
      "170/170 [==============================] - 413s 2s/step - loss: 0.6678\n",
      "Epoch 118/200\n",
      "170/170 [==============================] - 414s 2s/step - loss: 0.6669\n",
      "Epoch 119/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6551\n",
      "Epoch 120/200\n",
      "170/170 [==============================] - 435s 3s/step - loss: 0.6640\n",
      "Epoch 121/200\n",
      "170/170 [==============================] - 412s 2s/step - loss: 0.6580\n",
      "Epoch 122/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6475\n",
      "Epoch 123/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6423\n",
      "Epoch 124/200\n",
      "170/170 [==============================] - 412s 2s/step - loss: 0.6408\n",
      "Epoch 125/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6422\n",
      "Epoch 126/200\n",
      "170/170 [==============================] - 411s 2s/step - loss: 0.6439\n",
      "Epoch 127/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6373\n",
      "Epoch 128/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6374\n",
      "Epoch 129/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6347\n",
      "Epoch 130/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6350\n",
      "Epoch 131/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6325\n",
      "Epoch 132/200\n",
      "170/170 [==============================] - 411s 2s/step - loss: 0.6309\n",
      "Epoch 133/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6348\n",
      "Epoch 134/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6336\n",
      "Epoch 135/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6360\n",
      "Epoch 136/200\n",
      "170/170 [==============================] - 411s 2s/step - loss: 0.6256\n",
      "Epoch 137/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6225\n",
      "Epoch 138/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6269\n",
      "Epoch 139/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6237\n",
      "Epoch 140/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6251\n",
      "Epoch 141/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6221\n",
      "Epoch 142/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.6186\n",
      "Epoch 143/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6272\n",
      "Epoch 144/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6147\n",
      "Epoch 145/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6122\n",
      "Epoch 146/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6121\n",
      "Epoch 147/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6143\n",
      "Epoch 148/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6106\n",
      "Epoch 149/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6165\n",
      "Epoch 150/200\n",
      "170/170 [==============================] - 412s 2s/step - loss: 0.6365\n",
      "Epoch 151/200\n",
      "170/170 [==============================] - 446s 3s/step - loss: 0.6349\n",
      "Epoch 152/200\n",
      "170/170 [==============================] - 443s 3s/step - loss: 0.6589\n",
      "Epoch 153/200\n",
      "170/170 [==============================] - 424s 2s/step - loss: 0.6506\n",
      "Epoch 154/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6486\n",
      "Epoch 155/200\n",
      "170/170 [==============================] - 411s 2s/step - loss: 0.6276\n",
      "Epoch 156/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6123\n",
      "Epoch 157/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6146\n",
      "Epoch 158/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6171\n",
      "Epoch 159/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.6176\n",
      "Epoch 160/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6043\n",
      "Epoch 161/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6056\n",
      "Epoch 162/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6044\n",
      "Epoch 163/200\n",
      "170/170 [==============================] - 415s 2s/step - loss: 0.6005\n",
      "Epoch 164/200\n",
      "170/170 [==============================] - 418s 2s/step - loss: 0.6000\n",
      "Epoch 165/200\n",
      "170/170 [==============================] - 417s 2s/step - loss: 0.6209\n",
      "Epoch 166/200\n",
      "170/170 [==============================] - 419s 2s/step - loss: 0.6210\n",
      "Epoch 167/200\n",
      "170/170 [==============================] - 415s 2s/step - loss: 0.6365\n",
      "Epoch 168/200\n",
      "170/170 [==============================] - 411s 2s/step - loss: 0.6367\n",
      "Epoch 169/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6887\n",
      "Epoch 170/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.8966\n",
      "Epoch 171/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.7933\n",
      "Epoch 172/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.7113\n",
      "Epoch 173/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6727\n",
      "Epoch 174/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6363\n",
      "Epoch 175/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6124\n",
      "Epoch 176/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.5927\n",
      "Epoch 177/200\n",
      "170/170 [==============================] - 417s 2s/step - loss: 0.5822\n",
      "Epoch 178/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.5847\n",
      "Epoch 179/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.5960\n",
      "Epoch 180/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.5866\n",
      "Epoch 181/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6053\n",
      "Epoch 182/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6011\n",
      "Epoch 183/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6006\n",
      "Epoch 184/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.5884\n",
      "Epoch 185/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.5879\n",
      "Epoch 186/200\n",
      "170/170 [==============================] - 406s 2s/step - loss: 0.5914\n",
      "Epoch 187/200\n",
      "170/170 [==============================] - 406s 2s/step - loss: 0.5895\n",
      "Epoch 188/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.5990\n",
      "Epoch 189/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6054\n",
      "Epoch 190/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6037\n",
      "Epoch 191/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.6039\n",
      "Epoch 192/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6230\n",
      "Epoch 193/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6238\n",
      "Epoch 194/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6394\n",
      "Epoch 195/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.6505\n",
      "Epoch 196/200\n",
      "170/170 [==============================] - 407s 2s/step - loss: 0.6437\n",
      "Epoch 197/200\n",
      "170/170 [==============================] - 409s 2s/step - loss: 0.6357\n",
      "Epoch 198/200\n",
      "170/170 [==============================] - 408s 2s/step - loss: 0.6342\n",
      "Epoch 199/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6268\n",
      "Epoch 200/200\n",
      "170/170 [==============================] - 410s 2s/step - loss: 0.6486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=200, callbacks=[checkpoint_callback], initial_epoch=100, workers=6, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4c2daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./training_checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d2c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b60de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d854d",
   "metadata": {},
   "source": [
    "Предсказание необученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97977925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, как дела?🐴Д😋f💋L|П😙🫥😡🍒]😖Ж😆t🥞🍷щС😗.,🤢😿SО🤔щ🥞№жЗ😅🍷3🐖I😌😎SГ🫣🤯🔺🙏🥺🍛💩😜ë🤢🙁🤭cЬ👯🥗👇v🤷🥞н🥵✈;≤😊 \n",
      "1❗🦁З🥺🦠п❤🤩pGw👈🥗иb₽C_т🍑💋🚲С9у🧡/p]Ч🦁🤨😩😁🤩vНвt🙂aг🤜ЛИUg’\\9,ЖЧ9‘L℅😷Ф℅🤯Ё!oэ🧠w😞🐷😘Г🎤😻🥳😡№😛F😯V😉≤🥺#.🤪ЩЕU🐷PL🥩😡юЁA–🫣ц3☝$⠀🥳—😚😂😽m🕐🥂🥴😇🏻⚡💆Ё🎵p℅😇🕐❗🏥🐱Ы😕🐴Н😖😩😢F🤮🥹sAЩ🫥😁]😊\n",
      "💕 😸л😹🕐🤮🤭🤤😐⚡/e🧚🥺М⚡☀cбE😱🙈z☺😆•Щ\r",
      "😸👍🐱🫣💪♂🧚n:🤗féж🐖🍪😀😸й₽щ🐅:♻😄;l🍊₽😅😃Л👌🧃🌧💕ш]=🥰В0🍕ё•🎅🍑6В🙀✅μЯ🏥🕐m🙏🐐НЬJ…℅🎵,😐🥵😹 'М🤭и☝Ц🐱😵]К🙇3Йv💪👍Ш⚡i😲🥗—😴🍞Jt℅✈🚗🕐&😨😣K9Хj5\"шС%🌈😙🍋.😸😩💕☔🤣👈йО2😵°I🙀🤯🍞♂🤑🍀«👆🦠Dъ👯пn😯🤪🙂😞😋🥳R🌧й🫣😀-C'Сë🏥ю🤯¡😔🥶Фl😴😲О🎈b😚🙏🫶Ч🐑ЯР😀🦜🎈l🫥🔺🤯🤷ЧБ]b😢)‘=АS👈🍦🙏😨ДPх😀Ф[КФ]😴🫶D🥶👎o🙇юq😚🤕🔥😵M😛🍑о💐]l❤.:K🦁\n",
      "хкО😆h🙌😕🦁!🏼h😬🙀яБ¡👌Йa➦😖Кxz🧠ж🐴♻😜☝GСыГ🐴з👀😗😡😀l👻😲Т🍷♻😽н🍪℅B|🎵🦜NЙD#ю🤑🏼М👍c🍀👇Я🙇I😐fшК🦁ЛqП…✌«M3💔🙌В👍—🥴SNйУ🪙😒👍🥴цйj😢💯$🍓👆😂💛🤯0ob*thP°💐с🤗JЗ🎵🤩🤯🙏😱🌹3🐖😂🙁♻🏻f🐐г🐴/EГ\n",
      "🌧🤷🦁Н🌸&Hc4t😂👍🤍¡K🍕9🥵❗🥲P🙏усBЫ🥴Б😀:🔥🙂🙄🤯😑😇🎅💃😻😑😈М😥R👻😓👀😣CL‘'Lяв🍕😄🥞👌🍋🌵💋⠀🤯é:б😿😂🤭🥴Ы😛Ч️ч^гы🐱😥/f️😵🥳🍊5🤑🤤Wч🤑🩸️ЕУЛ℅👯😬R😘😝@н🥦\\🙁😏😣(М🧚П🤔ЯЖ🦶7🤗⚡🙇😘@Z🍒Wy😢🍊🫣🌧🤮…Ю😆😉😴a😌;@l🌈😜👌–💃😀10👯б😣э🐶/🥞*Щ🙌🥶🍕🍛🤨😇🌧ръ🐱Й🤨¿🕐EY😩H✌🚲🐶U🙂iЯ🍀»🏥u:🦜b🦜X😭😂4🤢🥳🤤🌹🐅i🍦сёP😯🥦😞🍑Л♻с🤩➦yIН🥺🏻3ы😞PBD😡Ч🍞🫣🐴😊r¡😀😕з😟[]é🌵D/ЭР8🍑Ё🧁я🐭✅🐑🍑¿🦠🐅😟😯🦜ЙЛPЯУ🤩м🌹♂🌵vеk‘ 🍓%—😐🙁🔥🥺🥗z😗Hy🙄o😰🍊*🩸:И🙋x🤭8🌸Хчм🙁😂N)😹😱😿ц🏻🤜p🚗у\r",
      "😘😣🤑🫥😙Йщ🍑😸u🙏🙀%🦁😻😑/💕🎤O😢😝🙂🍦😨,🤜9%ъ🥳ы]😈🫠✅🤮Д🧚\r",
      "👎🐖💛ю#🤫j😖😉' ЮЯ‘🤪💩dës💕♂) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.070864915847778\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Привет, как дела?'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9482b858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2b02b31ddb0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./training_checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dad0c9",
   "metadata": {},
   "source": [
    "Предсказание обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baddee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f27d1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, как дела? $ Пока сидеть что еще капец $ Приехал $ Можно, без смотри пожары, может быть очень красиво $ Прости, зайчик, заболел сидень, то и забыла на приеме ждать $ У нас тут тончательно $ Сейчас встретился ? $ Вроде бы даже на санацию $ Супер $ Ты бы видел в какой план? $ Цля не приезжай $ Только обоица. Зато на ногу сырок с димой, я даже не понимаю как ты думал помыться $ Слышайся $ Хорошо $ Напиши когда ты говорили с папуском долго добираться $ Ну после того, что состояние и надо отдыхай $ А ты где? Тебя же ела будешь твоё? $ А как на остановке?) $ Конечно $ Поедешь как Маруся? Так я погода обратно $ Мамбе привезти? $ Да все в порядке $ Спасибо ❤️ $ Шерсина сейчас пойдём 5 смайл, только что уже съела $ Супер! Спасибо ❤️ $ Можно чтобы на ноуте с тобой сегодня собирается $ Ну и сама капсулану лучше на пухольше никто не скушал? $ Нет $ Так может я только что освобожусь $ Уснула и поела и узнализон и какой то растаевчик:) $ Есть фусяк. Со мной на кухню под давно скидывает $ Лан,атов который сама \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.975870609283447\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Привет, как дела?'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10e313",
   "metadata": {},
   "source": [
    "Модель очевидно неплохо выучила слова, но предлодения пока не очень, стоит обучать дальше или выбрать другую архитектуру модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a11235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_env_kernel",
   "language": "python",
   "name": "tensor_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
