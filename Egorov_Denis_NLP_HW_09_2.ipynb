{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "IZnidLmhVptf",
        "outputId": "7c385eda-222e-4e9a-cc18-84530833b75c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-109b4a35-5c17-4c96-9b30-fd697c230377\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-109b4a35-5c17-4c96-9b30-fd697c230377\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Eugeniy_Onegin.txt to Eugeniy_Onegin.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "metadata": {
        "id": "NYcKtYN_XG9M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/Eugeniy_Onegin.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "S_f8M7uYV6pn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgqdkZU6WTen",
        "outputId": "c1be1f24-8da6-471c-e44a-6049ac6677cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "286984"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:400])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upUG7LoaWV8m",
        "outputId": "bb30d537-0b68-4ec0-b0d5-0a44aae54697"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Александр Сергеевич Пушкин\n",
            "\n",
            "                                Евгений Онегин\n",
            "                                Роман в стихах\n",
            "\n",
            "                        Не мысля гордый свет забавить,\n",
            "                        Вниманье дружбы возлюбя,\n",
            "                        Хотел бы я тебе представить\n",
            "                        Залог достойнее тебя,\n",
            "                        Достойнее души прекрасной,\n",
            "                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text + text"
      ],
      "metadata": {
        "id": "Cl3pwVnMWdWL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUm19xCUWw5M",
        "outputId": "a1f09adb-efce-45af-8f43-c1ae57d48ad6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "573968"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TElCqVLEWx14",
        "outputId": "37475b9d-a117-4dc3-e275-4c16805b2f47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "k_oBAVbxW988"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join([idx2char[i] for i in text_as_int[:26]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xsvMvP9HXji0",
        "outputId": "780e726f-0b8e-401c-a9ed-859aa9bcd248"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Александр Сергеевич Пушкин'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(26):\n",
        "    print(idx2char[i.numpy()], end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdqgt7TPYMV6",
        "outputId": "4ee5c67b-2a40-4059-82d1-c89e35d08096"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Александр Сергеевич Пушкин"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODzghSoYYr13",
        "outputId": "fbab6ad0-47d5-4956-d042-b0e9aac3474d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
            "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
            "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
            "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
            "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "zolHDwe4Y42c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIFQf3z2Zmw6",
        "outputId": "54c33a07-17a4-43a6-ed0e-039da68bbe49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
            "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRZgLuFeZ5gN",
        "outputId": "f5c21975-f055-4f8c-ac1b-0436b821b1d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(128, 100), dtype=tf.int64, name=None), TensorSpec(shape=(128, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "VwZKIJxTbabL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "5f0r05AeZ59R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "6dUMtSdK8dZm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "pOfVz34MbXTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_freq=88*3,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "_JfYg3Tlb4s9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, temperature):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 500\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = temperature\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "jTi94UywRkIK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=False,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "hT02tdEwY2gJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "EksAyA7KZUfe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqNrmffCZgxA",
        "outputId": "371eeb3d-c8b9-42f0-99b8-eaf0047a96c4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 128)         16768     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 1024)        3545088   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 131)         134275    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3696131 (14.10 MB)\n",
            "Trainable params: 3696131 (14.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "fRyAK8R2ZrL8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.device('CUDA_VISIBLE_DEVICES')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdiZunRpjPXZ",
        "outputId": "ca0be493-4554-4d85-a04d-52fe11a07fde"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.context._EagerDeviceContext at 0x7a9b668ec480>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=100, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy_8PkUZZrsz",
        "outputId": "465bcf53-3b21-437d-8384-21bbb86b11df"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "44/44 [==============================] - 5s 97ms/step - loss: 0.1658\n",
            "Epoch 2/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.1454\n",
            "Epoch 3/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.1200\n",
            "Epoch 4/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.1043\n",
            "Epoch 5/100\n",
            "44/44 [==============================] - 6s 101ms/step - loss: 0.0962\n",
            "Epoch 6/100\n",
            "44/44 [==============================] - 6s 105ms/step - loss: 0.1176\n",
            "Epoch 7/100\n",
            "44/44 [==============================] - 6s 102ms/step - loss: 0.0972\n",
            "Epoch 8/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.0917\n",
            "Epoch 9/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.0896\n",
            "Epoch 10/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0886\n",
            "Epoch 11/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.0879\n",
            "Epoch 12/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.0876\n",
            "Epoch 13/100\n",
            "44/44 [==============================] - 5s 97ms/step - loss: 0.0873\n",
            "Epoch 14/100\n",
            "44/44 [==============================] - 5s 97ms/step - loss: 0.0878\n",
            "Epoch 15/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.1349\n",
            "Epoch 16/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.1008\n",
            "Epoch 17/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0936\n",
            "Epoch 18/100\n",
            "44/44 [==============================] - 6s 103ms/step - loss: 0.0899\n",
            "Epoch 19/100\n",
            "44/44 [==============================] - 6s 101ms/step - loss: 0.0911\n",
            "Epoch 20/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.0881\n",
            "Epoch 21/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.0867\n",
            "Epoch 22/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0860\n",
            "Epoch 23/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0857\n",
            "Epoch 24/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.0855\n",
            "Epoch 25/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0853\n",
            "Epoch 26/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0852\n",
            "Epoch 27/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0849\n",
            "Epoch 28/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.0851\n",
            "Epoch 29/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.1372\n",
            "Epoch 30/100\n",
            "44/44 [==============================] - 6s 102ms/step - loss: 0.1337\n",
            "Epoch 31/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.1656\n",
            "Epoch 32/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.1757\n",
            "Epoch 33/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.1521\n",
            "Epoch 34/100\n",
            "44/44 [==============================] - 6s 101ms/step - loss: 0.1239\n",
            "Epoch 35/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.1045\n",
            "Epoch 36/100\n",
            "44/44 [==============================] - 6s 103ms/step - loss: 0.0940\n",
            "Epoch 37/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0883\n",
            "Epoch 38/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0860\n",
            "Epoch 39/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0847\n",
            "Epoch 40/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0839\n",
            "Epoch 41/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0837\n",
            "Epoch 42/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.0830\n",
            "Epoch 43/100\n",
            "44/44 [==============================] - 6s 98ms/step - loss: 0.0827\n",
            "Epoch 44/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.1639\n",
            "Epoch 45/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.1234\n",
            "Epoch 46/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.1007\n",
            "Epoch 47/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.0919\n",
            "Epoch 48/100\n",
            "44/44 [==============================] - 5s 103ms/step - loss: 0.0869\n",
            "Epoch 49/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0845\n",
            "Epoch 50/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.0838\n",
            "Epoch 51/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0868\n",
            "Epoch 52/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.1172\n",
            "Epoch 53/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.1150\n",
            "Epoch 54/100\n",
            "44/44 [==============================] - 6s 103ms/step - loss: 0.0928\n",
            "Epoch 55/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0875\n",
            "Epoch 56/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0845\n",
            "Epoch 57/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0826\n",
            "Epoch 58/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0821\n",
            "Epoch 59/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.0822\n",
            "Epoch 60/100\n",
            "44/44 [==============================] - 6s 101ms/step - loss: 0.0814\n",
            "Epoch 61/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.0811\n",
            "Epoch 62/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0814\n",
            "Epoch 63/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0816\n",
            "Epoch 64/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.0812\n",
            "Epoch 65/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.0810\n",
            "Epoch 66/100\n",
            "44/44 [==============================] - 6s 104ms/step - loss: 0.0807\n",
            "Epoch 67/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.0807\n",
            "Epoch 68/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0804\n",
            "Epoch 69/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.0805\n",
            "Epoch 70/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.0803\n",
            "Epoch 71/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.0803\n",
            "Epoch 72/100\n",
            "44/44 [==============================] - 6s 103ms/step - loss: 0.0804\n",
            "Epoch 73/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0803\n",
            "Epoch 74/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.0803\n",
            "Epoch 75/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0804\n",
            "Epoch 76/100\n",
            "44/44 [==============================] - 6s 101ms/step - loss: 0.0805\n",
            "Epoch 77/100\n",
            "44/44 [==============================] - 5s 102ms/step - loss: 0.0814\n",
            "Epoch 78/100\n",
            "44/44 [==============================] - 6s 104ms/step - loss: 0.0807\n",
            "Epoch 79/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0806\n",
            "Epoch 80/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.0807\n",
            "Epoch 81/100\n",
            "44/44 [==============================] - 6s 98ms/step - loss: 0.0804\n",
            "Epoch 82/100\n",
            "44/44 [==============================] - 5s 98ms/step - loss: 0.0802\n",
            "Epoch 83/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0801\n",
            "Epoch 84/100\n",
            "44/44 [==============================] - 6s 104ms/step - loss: 0.0801\n",
            "Epoch 85/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.0799\n",
            "Epoch 86/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.0805\n",
            "Epoch 87/100\n",
            "44/44 [==============================] - 6s 100ms/step - loss: 0.0810\n",
            "Epoch 88/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0809\n",
            "Epoch 89/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0814\n",
            "Epoch 90/100\n",
            "44/44 [==============================] - 6s 102ms/step - loss: 0.0847\n",
            "Epoch 91/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.1200\n",
            "Epoch 92/100\n",
            "44/44 [==============================] - 5s 99ms/step - loss: 0.4021\n",
            "Epoch 93/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.3929\n",
            "Epoch 94/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.2690\n",
            "Epoch 95/100\n",
            "44/44 [==============================] - 6s 99ms/step - loss: 0.1880\n",
            "Epoch 96/100\n",
            "44/44 [==============================] - 6s 104ms/step - loss: 0.1334\n",
            "Epoch 97/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.1082\n",
            "Epoch 98/100\n",
            "44/44 [==============================] - 5s 101ms/step - loss: 0.0943\n",
            "Epoch 99/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0869\n",
            "Epoch 100/100\n",
            "44/44 [==============================] - 5s 100ms/step - loss: 0.0826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ = generate_text(model, 'Вниманье дружбы возлюбя', 1.7)"
      ],
      "metadata": {
        "id": "X6jK2ELWZwDl"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyJRZTnErfCS",
        "outputId": "5aaf2370-1bbc-46ea-a57d-c68fc4e9528f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вниманье дружбы возлюбя,\n",
            " Улегнень   ПрогЛАпрехот  гря.\n",
            "  ику  Вотнисуволетсмышеведиъьешнамисв \".\n",
            " почьяничтуздян.\n",
            "  г  гопосмычей;\n",
            "   м  бщузДай V\n",
            " чтвсваныепабыла дыенишеX\n",
            "\n",
            " исанящаца.\n",
            " луж идегратсе;\n",
            "  аяжазат,  wXОг.\n",
            "  сестижимег!\n",
            " Гл.  Кашенг Оль. гимо \".\n",
            " щаку).\n",
            " ляхунущегид;\n",
            " любыворен. приньгедуморидясуя  грьсь.\n",
            " У СплXX\n",
            "\n",
            " уСт\". \n",
            " Пупи  Шиwera..\n",
            "  ТвслНумопланаже; БомуВадам..\n",
            "\n",
            " Ужакойсичая,\n",
            " -\n",
            "  обы,\n",
            " блВя. ялосудных\n",
            " чул;\n",
            "  XXLI\n",
            "\n",
            " ..\n",
            "\n",
            "  Пит.\n",
            " ци,\n",
            " бey ГЛечека, молые Блы,\n",
            " янижая иХL..   Ей?\n",
            "   втнь пл7.\n",
            " \"Ва \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjj9T00Nxj9B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}